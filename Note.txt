Transient State:
When an object of the Student class is created but not yet saved in the database.
The ID of this object is null in this state.
The object exists only in Java memory.
When save() is called:
First, it checks whether the object is new or has been saved before.
Criteria for detection: Is ID == null?
If yes → the object is new → uses EntityManager.persist().
If no → the object exists → uses EntityManager.merge().
What happens after persist()?
An INSERT is performed in the database.
The object changes state from transient to persistent.
An ID is created by the database for the object (e.g. 1).
Detached State:
After the transaction is committed and the EntityManager is closed, the object is no longer managed.
The object enters the detached state.
Always work only with the returned object after using save(). Depending on the circumstances, it may have been merged and a new object may have been returned.

Objective: Update a Student in the database using the save method
Step 1: Create and save a new entity
Here, a new object of the Student class is created and saved using save.
Behind the scenes, EntityManager.persist() is called.
The object is placed in a detached state; that is, it is detached from the context but its information is stored in the database.
Step 2: Update the entity information
What happens when a Detached entity is saved with save()?
When you have previously saved an entity (for example, Student) in the database and now keep it in Java memory as detached (i.e. outside Hibernates control) and want to save it again using the save() method (which uses EntityManager.merge() behind the scenes), Hibernate goes through the following steps:
1. Checking for the existence of an entity in the Persistence Context
Hibernate first checks:
"Is an entity with the same ID (e.g. id = 1) already managed in memory (the Persistence Context or 1st-level cache)?"
If there isn't one (which is usually the case):
Hibernate sends a SELECT to the database to read the record for that ID.
Result: Hibernate now loads a copy of Student with id = 1 from the database and puts it in persistent state.
Hibernate then copies the data from your detached entity to the newly loaded entity.
In other words: the state of the detached object is transferred to the persistent object.
2. Dirty Checking
Hibernate now checks whether the values of the persistent entity fields have changed since the initial load after copying the data.
If there is a change (such as changing the name Alisa to Alissa):
Hibernate detects that the entity has become "dirty", meaning it needs to be updated.
Hibernate then issues an UPDATE
3. Returning the new entity to the client
After executing merge():
Hibernate removes the persistent object from the context (i.e., detaches it).
The new version of that entity (which now has new information such as name = "Alissa") is returned as the output of the save() method.

Detached Entity (id=1, name="Alissa")   →  save() →  merge() →
→ SELECT student WHERE id = 1 →
→ Load persistent entity from DB →
→ Copy state from detached → persistent →
→ Dirty Checking detects change →
→ UPDATE student SET name='Alissa' WHERE id=1 →
→ Return new detached entity (updated)

Deleting a Detached Object
When delete(returnStudent) is executed, Hibernate faces three problems:
Problem 1: Does this entity actually exist in the database?
Hibernate needs to check whether the record actually exists.
Hibernate first uses EntityManager.find().
This causes a SELECT to be executed to find the record in the database.
Problem 2: The object cannot be deleted because it is Detached!
Hibernate can only delete Entities that are in Persistent state.
If it tries to delete the Detached object directly, we get an Exception:
IllegalArgumentException: Removing a detached instance
Solution: Using merge()
What does Hibernate do?
First it uses merge(returnStudent).
If an Entity with the same ID exists in the Context, it uses it.
If not, it SELECTs again and loads it from the database.
The returnStudent data is copied to that new Entity.
As a result, Hibernate has a new Entity in the Persistent state.
The final step: the actual deletion
Now that we have an Entity in the Persistent state:
Hibernate removes it using EntityManager.remove() .
This executes a DELETE statement in the database.
Important:
The returnStudent object is not deleted.
Instead, Hibernate:
Creates a new copy of it,
Merges the data,
And deletes the new copy.
After the deletion, returnStudent is still in the Detached state!

Are there always two SELECTs?
No. If the following conditions are met, Hibernate may just do a SELECT or even delete without a SELECT:
Case 1: If the same entity is already in context (e.g. still in context after findById())
In that case Hibernate uses the same instance.
No need for merge() and new SELECT.
Just remove() and finally a DELETE.
Case 2: If the entity is in Persistent state (e.g. still alive in the same transaction)
Hibernate directly does remove(), without SELECT

What is Spring Data Commons?
It provides a common foundation for all Spring Data projects, whether for relational or non-relational (NoSQL) databases.
This project defines the main and public Repository interfaces such as Repository, CrudRepository, and PagingAndSortingRepository.

Spring Data JPA and Spring Data Commons Relationship
Spring Data JPA is built on top of Spring Data Commons.
JpaRepository, which is used in JPA projects, itself inherits from PagingAndSortingRepository, which belongs to Spring Data Commons.
Therefore, in addition to CRUD methods, it is possible to use methods related to pagination and sorting.
You can also use CrudRepository or PagingAndSortingRepository directly if you do not need all the features of JpaRepository.

Spring Data MongoDB and Spring Data Commons Relationship
Spring Data MongoDB is also built on Spring Data Commons, similar to JPA.
MongoRepository in Spring Data MongoDB is similar to JpaRepository but is specific to working with MongoDB databases.
This makes working with MongoDB and JPA very similar in terms of API and makes it easier to switch between them.
Spring Data's overall philosophy
Spring Data aims to provide a unified and database-independent programming model.
This allows developers to migrate from relational databases to NoSQL and vice versa without much change in the code.

Creating a custom Repository with custom methods from different interfaces (such as CrudRepository and JpaRepository)
We want to create a Repository called MyCustomRepository that has only these 3 methods
How do we create a Repository that has only these three methods?
MyCustomRepository should only inherit from the base Repository (marker interface) and then define these three custom methods inside itself.
Spring Data JPA creates an implementation for this Repository dynamically (Dynamic Proxy) at runtime that implements those methods.
Why might we want to have only three methods? Why not use the full JpaRepository?
You might not want all the methods to be available to developers.
For example, you don't want to have deleteById because you use soft delete.
You don't want to give the most complete access to specific modules or external APIs.

Named Queries
If you want to define a query in the Entity itself:
Named JPQL Query in the Student entity:

@NamedQuery(
  name = "Student.findByName",
  query = "SELECT s FROM Student s WHERE s.name = :name"
)

Named Native SQL Query:

@NamedNativeQuery(
  name = "Student.findByNameEndingWith",
  query = "SELECT * FROM student WHERE name LIKE %:pattern",
  resultClass = Student.class
)

By default, JUnit 5 creates a new object of the test class for each test method. That is, if your test class has 3 methods, 3 instances of the class will be created.
@TestInstance(TestInstance.Lifecycle.PER_METHOD)

If you want only one object of the test class to be created to execute all test methods, you should write this annotation:
@TestInstance(TestInstance.Lifecycle.PER_CLASS)
The test class is instantiated only once.
You can use non-static fields in the @BeforeAll and @AfterAll methods.

Suppose you want to use a Repository (say StudentRepository) initialized with @Autowired.
In the default case (PER_METHOD):
Because @BeforeAll must be static, you cannot use studentRepository (because static methods do not have access to non-static fields).
But with PER_CLASS:
The @BeforeAll method can be non-static, so you have access to the studentRepository field.
This means you can prepare the database with Repository before running the tests.

If the Repository method is annotated with @Query, the same query will be executed. This has the highest priority.
If @Query is not present, Spring Data JPA checks whether a Named Query with the same name is defined (for example, in the entity class with @NamedQuery). If so, it will be executed.
If neither @Query nor Named Query is defined, then Spring Data JPA attempts to automatically generate and execute a query based on the method name (i.e., a Derived Query).

Query by Example (QBE) is a way in which:
Instead of writing a manual query (like findByLastNameAndLevel)
You create an object of the class you want and pass it to JPA
JPA will search the database for records that are similar to that object
This object is called: "Probe" (like an example that we need to search for)

Step 1: Create a Probe
Suppose you want to find all users who:
lastName = "Smith"
level = 2

User probe = new User();
probe.setLastName("Smith");
probe.setLevel(2);

Step 2: Create an Example
Example<User> example = Example.of(probe);

Step 3: Search with Repository
List<User> result = userRepository.findAll(example);

This line says:
Go get all users with lastName = "Smith" and level = 2.

The problem occurs when you use primitive fields
For example, if you just write:
User probe = new User();
probe.setLastName("Smith");
You think level will be ignored, but no! You are wrong.
Why?
Because level is an int and the default value of int is 0.
It is actually telling JPA:
Look for lastName = "Smith" and level = 0!
And since there is no user with level = 0 in our database, the result is: empty list.
What is the solution? Using ExampleMatcher
We are telling JPA:
Please ignore level, even though it has a default value!

ExampleMatcher matcher = ExampleMatcher.matching()
.withIgnorePaths("level"); // Ignore level field
Example<User> example = Example.of(probe, matcher);
List<User> result = userRepository.findAll(example);

Ignore a specific field .withIgnorePaths("level")
Ignore case .withIgnoreCase()
Match first, middle, or last name? .withMatcher("firstName", startsWith()), contains(), endsWith()

A transaction is a set of operations that are executed as an atomic unit. Either all of those operations are successful, or none of them are successful and all are rolled back to the previous state.
Suppose you have a ticket booking application. When a user makes a ticket reservation, a seat must first be assigned to him and then the payment must be made. If the payment fails, the seat must not be reserved for him. This is possible using a "transaction".
Commit:
This means that all the operations within the transaction have been completed successfully and now the database must save those changes and make them visible to other users.
Rollback:
This means that an error has occurred and the changes made must be rolled back to the previous state; no trace of that operation remains in the database.

Basic properties of a transaction (ACID)
1. Atomicity
All operations in a transaction must be completed completely or none of them.
Example:
Suppose you are transferring money from your bank account to someone else's account.
The two main operations are:
Money decreases in your account
Money increases in the other party's account
If only one of them happens (for example, money decreases in your account but does not reach the other party's account), it is a disaster!
Atomicity guarantees that either both happen, or neither happens.

2. Consistency
The database must remain in a valid state before and after the transaction.

Example:
Suppose there is a rule in the database that the account balance should never become negative.
If this rule is violated during the transaction, the entire transaction must be canceled.
Consistency means that after the transaction is executed, the data is still consistent with the rules of the system.

3. Isolation
Transactions should not affect each other or have incomplete results.
Example:
Suppose two people are simultaneously reserving a specific seat in a ticketing system.
If the transactions are not isolated, both people could end up buying the same seat!
Isolation prevents this from happening. Each transaction is executed as if in its own space.

4. Durability
Once a transaction is successfully committed, the data should persist forever, even if the system crashes or loses power.
Example:
If a user purchases a ticket and the system shuts down after a few moments, the ticket should still be recorded in the database.
Durability means that the database retains that information as soon as the transaction is committed.

One of the first things that comes to mind when talking about getting a database connection is that it is a resource-intensive process.
If your application does not use a connection pool, every time a database operation is performed, that operation will be relatively slow and will not be suitable for your application in terms of performance.
Here’s how it works: when a client sends a request to perform an operation on the database (e.g., register a ticket), if your Spring application is not configured to use a connection pool, behind the scenes, the application opens a new connection to the database; this is relatively time-consuming.
And when the database operation is finished processing, the server has to close this connection before the response can be returned to the client; this is also slow.
And only after the connection is closed, the response is returned to the client.
So if your application does not use a connection pool, every database operation will be very slow; Because opening and closing a connection are two very time-consuming and expensive processes.

In production environments, you absolutely do not want to have an application that does not use a connection pool.
And when your application is properly configured to use a connection pool, when the server starts, it automatically pre-empts several connections to the database.
As a result, you have a pool of connections ready to be used by clients for database operations.
Now, if we have the same request as before – say, saving a ticket – this time, instead of opening a new connection, the server just borrows one of the free connections in the pool.
A connection that is not currently busy responding to another request.
Then, it hands your request over to that same connection to perform the ticket registration operation or any other database operation.
So this time, with the connection pool, the ticket registration operation will not be as slow as before; Because we didn’t need to open a new connection.
We just had to borrow an existing one.
So, this time the operation is performed significantly faster.
And when that connection is done with the database operation (in our example, registering a ticket), the server doesn’t close that connection.
Instead, it simply returns it to the connection pool to be used to respond to other client requests.
That is, the connection that was borrowed from the pool is not closed, but released and returned to the pool and remains open.
This will make your database operations even faster, because as we said, closing a connection is also a slow process.

Spring Boot uses a connection pooling library called HikariCP by default.
And now when you mark a method in a service with the @Transactional annotation, if you use Resource-local (i.e. you only have one datasource), Hibernate immediately acquires a database connection at the very beginning of the transaction, i.e. immediately after the transactional method is called.
This connection is for a JDBC transaction that is acquired at the beginning of the method execution – even before any actual operations are performed on the database.
This is not good, because acquiring a connection and not using it can hurt the performance of your application.
You should acquire a connection only when you are actually going to perform a database operation, not at the beginning of the method.
For example, let's say we have a method in a service that is annotated with @Transactional.
When this method is called from the client, a transaction is immediately started with the EntityManager.
And what we are going to examine in this session is that at this very moment, even before doing anything, Hibernate gets a connection to the database.
Suppose in line 17 we do something that takes time, but does not require a database connection.
For example, we simulate a task that takes 40 seconds using Thread.sleep(40000).
If you look at the console log at this moment, you will see that even in the case where line 17 does not require any database, HikariCP has an active connection.
That is, out of 10 connections in the connection pool, one of them is active, while we do not need it - which is not good from a performance point of view.
You should get the connection exactly when you really need it - for example, in line 19 when we find a ticket from the database - and not earlier.
Here you may ask:
Why does this happen?
Why is the database connection taken right at the beginning of the method and why is this process not delayed until line 19 when the database operation is performed?
The answer is that in resource-local JPA transactions, the database connection is taken immediately.
Because Hibernate needs to check the auto-commit status of the underlying JDBC connection; and if it is enabled, disable it.
That is why the connection must be taken at the beginning of the method; because without this, the transaction cannot be managed as an atomic unit.

That is, each database operation is stored immediately, independently, and permanently; each operation has its own transaction.
For example, if a ticket reservation requires two operations—first assigning a seat, second receiving the payment—in auto-commit mode, each of these operations has a separate transaction.
And this is not what we want at all.
We want either both to happen, or neither (atomicity).
And this happens if auto-commit is disabled.
Hibernate does exactly the same thing: it disables auto-commit at the beginning of the method, and to do this, it has to acquire a connection.
So, the solution to this problem is to postpone the process of acquiring the connection until the first database operation is reached (lazy connection acquisition).
And this can be done very easily, just set the following two properties in the application.properties file:

spring.datasource.hikari.auto-commit=false
hibernate.connection.provider_disables_autocommit=true

These two properties make Hibernate disable auto-commit by itself, instead of connecting at the beginning of the method to disable auto-commit.
As a result, Hibernate no longer needs to connect at the beginning of the method.
Rather, this connection is only taken when the actual operation on the database begins.
The first property is related to Hikari, and the second is related to Hibernate.
Be sure to enable both, because just one is not enough.
To see the HikariCP logs, you can also set the corresponding log level to trace:

logging.level.com.zaxxer.hikari=TRACE

With this, the connection pool status will be logged to the console every 30 seconds.

Let's say we have an entity called Guide that has an ID, a staff ID, a name, and a salary.
This entity is mapped to the guide table, which currently contains two guides.
We also have a GuideRepository that we use to store this entity. This repository inherits from JpaRepository.

Now the question is:
What will be the result of executing this method written here?
In this service method (on line 18), we first try to find the guide with the ID 1. Then, in the next line, we increase his salary from 1000 to 2500.

This method is marked with the @Transactional annotation, which means it runs in a transaction context;
In such a way that when the method starts executing, a transaction is started and at the end of the method, the transaction is committed or rolled back in case of an error.

In your opinion, can this method change the permissions of the user with ID 1 to 2500?
The answer is yes; the permissions of this user will be changed to 2500.
Why? Because by default, Spring transactions work in read/write mode, and in this mode, dirty checking is automatically performed when the transaction is committed.
In this process, Hibernate checks whether any data has changed, and if so, issues an SQL update statement that updates the permissions in the database to 2500.

The first thing we need to know is that the default value of the readOnly attribute in the @Transactional annotation is false, which means that the transaction is executed in read/write mode.
In this case, automatic dirty checking is performed on commit.

Like this:
When the select statement is executed on line 18 to get the Guide with ID 1, in addition to receiving a Guide object, a snapshot or loaded state of the initial state of that object is also maintained.
This snapshot is used by Hibernate during dirty checking to find out if anything has changed in the object.
During a read/write transaction, the loaded object is managed in the persistence context by the EntityManager.
That is why dirty checking can be performed.
If the state of the object was not managed, dirty checking would not be performed (which happens in read-only transactions).

When we get to line 19 and change the permissions to 2500, this change is recorded in memory (Java memory).
When committing the transaction, Hibernate performs dirty checking:
It compares the current state of the object with the original snapshot.
If it sees a difference (here the permissions have changed from 1000 to 2500), it issues an update command to bring the database into line with the new state.
This will change the permissions of the guide with ID 1 in the guide table to 2500.

These snapshots consume memory.
Dirty checking can be time-consuming, as Hibernate has to check all the objects in the persistence context.
So if you put unnecessary objects into the persistence context, it will cause a performance degradation of the application.
So always try to load only the data that you intend to change in read/write transactions.

Now about read-only transactions
In the second scenario, we do the same thing (load the directory with ID 1 and change the permissions to 2500), but this time our method has @Transactional(readOnly = true).
What happens in this case?
Answer:
This time the permissions are not changed to 2500.
Why?
Because in a read-only transaction,
dirty checking is not done
The initial snapshot of the objects is not kept
Flush (applying changes) is also not done at the end of the transaction
In fact, in read-only:
When data is read from the database, it is registered as read-only in the persistence context and Hibernate does not manage it.
Since Hibernate does not manage it, there will be no dirty checking operation either.
So at the time of commit, no update command is issued and the permissions value in the database remains the same as 1000.
From Spring's perspective, this is a performance optimization; because when you are fetching data for reading only and not intending to change it, there is no need for dirty checking or snapshots.
This saves memory and time.

In read/write transactions → dirty checking is enabled, snapshot is kept, changes are applied to commit.
In read-only transactions → dirty checking is disabled, snapshot is not kept, flush is not performed.
So if you are sure that you only want to read the data and not change it, be sure to use read-only to get better performance.
If you want, I can explain this text to you in a simpler or more concise way — tell me how I can help you.

We are talking about Transaction Propagation. What happens when a method marked with @Transactional calls another transactional method.
For example:
The foo() method in the Service1 class calls the bar() method in the Service2 class.
Both methods are transactional (i.e. they are marked with @Transactional).
In this case, we want to see:
Will bar() run in its own transaction or in the same transaction that foo() created?
Will we have only one transaction or two separate transactions?

We have a service called ItemService whose job is to save the Item entity.
In this class, we have a method called saveItem() that is marked with @Transactional.
That is: when this method is executed, a transaction is started.
When this method is called:
A new transaction is started.
First, the Item entity is saved.
Then, the log() method of the LogService class is called.
In the LogService class, we also have a method called log(), which is also annotated with @Transactional.
In this method:
A Log object is created,
and saved to the database.
When saveItem() is called, and from within it, another transactional method (log()) is called, what exactly happens to the transaction?
Does LogService.log() run in its own transaction?
Or in the same transaction that ItemService.saveItem() created?
And this is where the concept of Propagation comes into play!
In the Spring documentation, propagation is a property of the @Transactional annotation, and its default value is:
Propagation.REQUIRED

What does that mean?
If we don't specify a propagation value, by default:
The second method (such as log()) runs in the same transaction that the first method (such as saveItem()) created.
So in this example, both Item and Log run in a common transaction.
Suppose we have the following code in the ItemService class:
@Transactional
public void saveItem() {
itemRepository.save(item);
logService.log();
}

And in LogService:
@Transactional
public void log() {
logRepository.save(log);
throw new RuntimeException("Something went wrong!");
}

What happens to the entire transaction when log() throws an error (RuntimeException)? Does the saved item get rolled back?
Since both the saveItem() and log() methods are marked with the default value @Transactional(propagation = REQUIRED), they are executed in the same transaction.
So if something goes wrong in the log() method and an error is thrown, the entire transaction is rolled back; that is, the data stored in the itemRepository is also rolled back and not saved.
This default behavior is usually what we expect: either everything is saved successfully, or nothing is saved.
This prevents inconsistencies.

When we use Propagation.REQUIRES_NEW, a new, independent transaction is started, and if there is currently an active transaction, that transaction is temporarily suspended until the new transaction is executed.

@Transactional
public void saveItem() {
itemRepository.save(item);
logService.log();
}

@Transactional(propagation = Propagation.REQUIRES_NEW)
public void log() {
logRepository.save(log);
throw new RuntimeException("Something went wrong in log!");
}

The log() method runs in a completely separate transaction.
When an error is thrown in log(), only the transaction for that method is rolled back.
The saveItem() transaction is not affected and is committed.
That is, the item is saved but the log is not saved.
If the exception is passed from log() to saveItem() and saveItem() also crashes because of that error, then the saveItem() transaction will also be rolled back!

To prevent this, we need to catch the exception:

@Transactional
public void saveItem() {
itemRepository.save(item);
try {
logService.log();
} catch (Exception e) {
// Log the exception, but don’t re-throw
}
}

When using Propagation.NESTED:

If there is currently an active transaction, the method will be executed as a nested transaction.

If there is no active transaction, it will behave exactly like REQUIRED and a new transaction will be started.

What does nested transaction mean?
A savepoint is created in the current transaction.
If a NESTED method fails, it will only rollback up to that savepoint, not the entire parent transaction.

@Transactional
public void saveItem() {
itemRepository.save(item);
logService.log();
}

@Transactional(propagation = Propagation.NESTED)
public void log() {
logRepository.save(log);
throw new RuntimeException("Logging failed!");
}

If the log() method fails (i.e. throws an Exception), only the operations inside that method will be rolled back.
The rest of the transaction in saveItem() (i.e. saving the item) will continue and be committed.
But only if we catch the exception in saveItem() and don't let it escalate.
For NESTED to work:
Your database and transaction implementation (i.e. DataSource) must support Savepoints.
Otherwise, NESTED may behave like REQUIRED or even encounter an error.

New transaction?
REQUIRES_NEW -> Yes, completely separate
NESTED -> No, child of parent transaction
Will parent transaction abort?
REQUIRES_NEW -> Yes
NESTED -> No

A method marked with Propagation.NOT_SUPPORTED should not be executed within a transaction.
If there is an active transaction when the method is executed, it is temporarily suspended.
When you want a piece of code to run without a transaction. For example:
Code that is just for reading data and you don't want the transaction to affect it.
An operation that is not transaction-dependent and should not be rolled back.

@Transactional
public void saveItem() {
itemRepository.save(item);
noTxService.printStats(); // Not transactional
}

@Transactional(propagation = Propagation.NOT_SUPPORTED)
public void printStats() {
// This method runs outside a transaction
System.out.println("Reading stats...");
}
Here, printStats() runs without a transaction even if saveItem() is in a transaction.

A method marked with Propagation.NEVER must not be executed in a transaction.
If there is an active transaction when it is executed, an exception is thrown.
To prevent the incorrect execution of a method inside a transaction.
For example, when you know that executing the method inside a transaction may cause data inconsistencies or record locking.

@Transactional(propagation = Propagation.NEVER)
public void sendEmail() {
// If executed in a transaction, an error is thrown
emailService.send(...);
}

A method marked with Propagation.MANDATORY must be executed inside an active transaction.
If there is no transaction when it is executed, an exception is thrown.
To ensure that a piece of code is only executed within the context of the current transaction.
For example, to make changes that should not be made independently.

@Transactional(propagation = Propagation.MANDATORY)
public void updateStock() {
// If called without a transaction, an error will be thrown
stockRepository.update(...);
}

When do we use pessimistic locking?
When multiple queries are executed on the same data in the same transaction.
For example, suppose we have a guide table and we want to generate a report from it:
Print a list of names and rights of all guides.
Also print the total of the rights.
To do this, we write two simple JPQL queries:
First query: Get the names and rights of all guides.
Second query: Get the total rights with the SUM function.
In the service:
First, we execute the first query and print the data.
Then, we execute the second query and print the total.
No-lock problem
There is a time gap between the execution of the first and second queries.
If during this time another user (e.g. User Y) changes the rights of one of the guides, the total returned by the second query will be different from the total of the initial list and the report will be incorrect.
Solution: Pessimistic Lock
To avoid this problem, we put a pessimistic read lock on the first query.
This lock causes:
Other users to be able to read the data but not to change it until the current transaction is complete.
When the transaction is committed, the lock is released.
This allows User X to get the report with the fixed data and User Y's changes are not applied until the end of the transaction.
Simulating two concurrent users
A thread for User X to get the report.
A thread for User Y to try to change the rights of a guide.
When the read lock is active, User Y's attempt to change will cause a PessimisticLockException.
Difference between Read Lock and Write Lock
Pessimistic Read Lock: Prevents others from changing the data, but allows reading the data.
Pessimistic Write Lock: Prevents others from reading and changing the data until we make our own changes.
Important Note: Although this method prevents inconsistencies, it can reduce system performance and scalability due to the use of real database locks, so its use is not very common.
The database blocks request Y.
Update Y will not be executed until transaction X has completed (committed or rolled back).
If this wait takes longer than the configured lock timeout, an exception such as PessimisticLockException or SQL timeout exception is thrown.

What is the transaction isolation level?
The isolation level determines how much and when data modified by a transaction is visible to other transactions.
Isolation is the extent to which transactions can operate independently and separately from each other.
But should transactions always be completely isolated from each other?
Ideally, yes. But in practice, creating complete isolation severely reduces the performance and scalability of the system.
The higher the isolation, the more secure and accurate the data will be, but the slower the system will run.
Suppose we have a table called guide with three rows (three guides).
User 1 opens transaction 1 and gets a list of the names and rights of the guides.
Before user 1 commits his transaction, user 2 opens transaction 2 and changes the rights of guide number 3 to 4000 and adds a new guide.
Now, what will happen if user 1 runs the same query again?
Do user 2's changes in transaction 2 affect user 1's results?
The answer depends on the isolation level set in the database.
Final example question:
User X creates a report: first reads the data (line 20), then prints the report in a loop, and finally calculates the total rights (line 25).
In between these two steps, user Y changes the rights of guide number 3 to 4000.
Question: What is the total rights printed by user X?
Answer:
If we use MySQL with default isolation (Repeatable Read), the report for user X will be 8000 because the data read in the transaction remains constant until the end of the transaction.
If we use H2 with default isolation (Read Committed), the report for user X will print 9000 because the changes of user Y are visible after the commit.
How to solve this problem?
We can set the transaction isolation in Java code using the annotation @Transactional(isolation = Isolation.REPEATABLE_READ).
This will make the current transaction behave as Repeatable Read and the data will remain constant even if the default isolation of the database is Read Committed.

Common Problems with Low Isolation
When there is insufficient isolation, several problems can occur:

Dirty Read
Transaction A reads data that transaction B has changed but has not yet committed (it may be rolled back).
In this case, transaction A reads data that may never go to the database at all.

Non-repeatable Read
Transaction A reads data, then transaction B changes that data and commits, then transaction A reads the same data again and the value is different.
That is, the data has changed between the two reads of a transaction.

Phantom Read
Transaction A reads some rows with a specific condition, then transaction B adds new rows that meet the condition and commits.
Now if transaction A reruns the query, it will see new rows (that were not there before).

The importance of choosing the right isolation level
If your work is sensitive to simultaneous changes and data errors (such as a banking or accounting system), it is better to choose a higher isolation level (Serializable or Repeatable Read).
If your system requires speed and fast response and some small and transient changes are acceptable, you can use lower levels.
There is always a balance between data accuracy and system efficiency.

The users table currently has five records:
3 level 1 users
2 level 2 users
The question is: How can we delete all level 1 users?
If we want to delete level 1 users, we can add a method like this:
@Transactional
long deleteByLevel(in
This method:
Takes the value of level.
Returns the number of records that were deleted.
Spring Data JPA automatically builds the appropriate query.t level);
An important point about transactions
Because this method modifies data (deletes), it must be executed inside a transaction.
That is why we add @Transactional.
By default, Derived Query methods (such as deleteByLevel) are not executed in a transaction.
If you do not add it, Spring will throw an error:
Transaction is required for this operation.
Suppose we have a method in UserService:
public void deleteUsersByLevel() {
long count = userRepository.deleteByLevel(1);
System.out.println(count);
}
When this method is executed:
Three level 1 users are deleted.
The value 3 is printed.
But three DELETE statements are executed (not one statement).
That is, the records are deleted one by one.
Problem with the first method
If the number of users is large, deleting one by one becomes slow.
Sometimes we want to delete all of them with one DELETE statement.

Second method: Bulk deletion
For this, we use @Query + @Modifying:
@Transactional
@Modifying
@Query("DELETE FROM User u WHERE u.level = :level")
int deleteInBulkByLevel(@Param("level") int level);
Features:
Only one DELETE statement is executed.
The data is deleted directly from the database (Persistence Context is not loaded).
@Modifying is required because our query is not SELECT.
@Transactional is still required.

Important difference between the two methods
First method (deleteByLevel):
Records are loaded into Persistence Context.
Deletion is done one by one.
Lifecycle events (like @PostRemove) are triggered.
Second method (deleteInBulkByLevel):
Records are deleted directly from the database.
Only one DELETE statement is executed.
No Callback is executed.

Example with Callback
On User:
@PostRemove
public void afterDelete() {
System.out.println("User with ID " + id + " deleted");
}
If we use the first method → a message is printed for each deleted user.
If we use the second method → no message is printed.
Reason for choosing each method
If you need more speed → Method 2 (Bulk).
If you need to execute Callback or special logic after deletion → Method 1.
Method 1: Delete level 1 users → three DELETE commands and execute Callback three times.
Method 2: Delete level 2 users → one DELETE command and no Callback execution.

By default, when you annotate a method with @Query, Spring Data assumes that the query is read-only and returns the result (SELECT).
When you add @Modifying, you are essentially telling Spring Data:
"This query is not supposed to return anything, but rather to modify the database (update/delete/insert/ddl)."
When you use @Modifying, Spring Data can set the method output to the number of records modified
The bulk operation is performed directly on the database, bypassing the Persistence Context.
If a record is already loaded into memory (Persistence Context), it will not be synchronized with the database value.
@Modifying handles this behavior so that after the query is executed, the Context needs to be cleared or synchronized (with clearAutomatically = true).
@Modifying(clearAutomatically = true)
@Query("UPDATE User u SET u.isActive = false WHERE u.level = :level")
int deactivateByLevel(@Param("level") int level);

In the service we have a @Transactional method that:
calls findByIsActive(false) — this is a SELECT and loads the inactive entities into the Persistence Context (e.g. users with id = 3 and 4).
Then it calls a repository method that performs a bulk delete with @Modifying and a JPQL DELETE ... WHERE u IN :users (a single DELETE statement to the database).
Question: What will be the result of executing this method?
When the service method is executed, Spring opens a transaction and a Persistence Context for it (due to @Transactional in the service).
Executing findByIsActive(false) => a SELECT is sent and the corresponding entities (e.g. id=3 and id=4) are placed in the EntityManager (persistence context) memory.
Executing the @Modifying method with JPQL DELETE FROM User u WHERE u IN :users => A single DELETE statement is sent to the database, deleting the corresponding rows in the table.
Note: This statement is executed directly on the database and does not delete or update the entities in the persistence context by default.
Result: The database has deleted the rows, but the managed versions of the same records still exist in the persistence context — unless the context is explicitly deleted.

1) Lifecycle callbacks (@PostRemove, etc.)
In bulk operations (JPQL DELETE/UPDATE with @Modifying), no lifecycle callbacks are executed.
That is, @PostRemove is not executed on the entities because JPA did not delete them via the persistence context — the deletion is done at the database level.
2) @Modifying has two important attributes: flushAutomatically (default false): If true, all pending changes in the persistence context are flushed before the query is executed (e.g. em.flush()). clearAutomatically (default false): If true, the persistence context is cleared after the query is executed (e.g. em.clear()) to clear managed instances and avoid inconsistencies. 3) The main problem when clearAutomatically=false is that after a bulk delete, the objects are still managed and if you continue to work with them in the same transaction (e.g. change their value), an inconsistency occurs (the database does not have that row but you still have a managed entity in memory). This can cause unexpected behavior or errors later on (e.g. updating arguments, a subsequent flush, or business logic assuming the deleted entity no longer exists). 4) What is the difference between deleteAllInBatch() (from JpaRepository)?
This method also executes DELETE commands in batch and does not call callbacks (similar to bulk behavior).
Important difference: The prepared deleteAllInBatch() methods usually do not have the clearAutomatically or flushAutomatically option. This means that after execution, the persistence context is not cleared unless you do it yourself. So there is a risk of stale values unless you call em.clear() yourself.

1) If you want to avoid problems after bulk delete — the easiest way:
Use @Modifying with clearAutomatically=true and (if needed) flushAutomatically=true in the repository:
@Modifying(clearAutomatically = true, flushAutomatically = true)
@Query("DELETE FROM User u WHERE u IN :users")
int deleteInBulk(@Param("users") List<User> users);
flushAutomatically=true: any pending changes (that are in the persistence context) are flushed before DELETE is executed.
clearAutomatically=true: after DELETE is executed, the persistence context is cleared and the "deleted" managed instances are no longer in the context.

2) Manual solution in the service (if the repository does not have this option)
@Transactional
public void deleteInactive() {
List<User> users = userRepository.findByIsActive(false);
if (users.isEmpty()) return;
userRepository.deleteInBulk(users); // Assumption: This method performs bulk delete
entityManager.flush(); // Synchronization (optional if repository does not flush)
entityManager.clear(); // Clear persistence context to avoid stale entities
}

3) If you want to not load entities at all before deleting (more efficient and safer)
Instead of find then deleteInBulk, write a delete method directly based on condition:
@Modifying(clearAutomatically = true)
@Query("DELETE FROM User u WHERE u.isActive = false")
int deleteAllInactiveDirectly();

This method:
Does not load entities from scratch.
It only performs a DELETE.
If you set clearAutomatically=true, the persistence context is cleared.
4) Be careful with the IN parameter :users
If the users parameter is an empty list, some JPA providers (or databases) may throw an error (SQL like IN ()). So it is better to check before calling the method if (users.isEmpty()) return 0;.

Pending changes are changes that have been made to entities but have not yet been applied to the database.
For example, when you change an entity (user.setName("Ali")) or add a new entity (entityManager.persist(user)), these changes are currently only stored in memory (Persistence Context).
The database is unaware of these changes until one of the following events occurs:
Flushing (automatic or manual)
End of transaction and commit
Execution of a query that requires synchronization of context and database

We want to update the level of all active users whose registration date is before January 1, 2020.
We have a User entity class that contains the fields id, version, username, registrationDate, level, and isActive.
The version field is annotated with @Version to enable versioning or optimistic locking.
We have an UPDATE query in the UserRepository that is annotated with @Modifying.
This query increments the level of all active users who registered before a specific date by one. The date is passed as a method parameter.
In the users table, we have 5 users, and only the users with ID=2 and ID=5 meet the above conditions.
In the foo service method, we call this repository method and pass in the date 2020-01-01.
The level value of both users (ID 2 and ID 5) is updated to 3, but the version value remains zero and does not change to 1.
When we directly update the records of an entity in the database, by default the version value does not change.
The version value is automatically incremented only when we modify the entity through Hibernate and inside the Persistence Context.
But here, since we are using @Modifying, a Bulk Update is performed which changes the records directly in the database and does not enter them into the Persistence Context. So Hibernate is not aware of this change and does not update the version.
In fact, the generated query only changes the level column and does not touch the version.
To solve the problem, we need to use the versioned keyword in the JPQL query (a Hibernate-specific feature).
This will automatically increment the version column by one when we do a Bulk Update.
@Modifying
@Query("update User u set u.level = u.level + 1 where u.isActive = true and u.registrationDate < :date")
We modify it like this:
@Modifying
@Query("update versioned User u set u.level = u.level + 1 where u.isActive = true and u.registrationDate < :date")
By re-executing the foo method:
The level value of both users (ID 2 and ID 5) will be incremented by one.
The version value of both will be incremented by one (from 0 to 1).
@Version only works when the entity is loaded into the Persistence Context and modified via Hibernate.
Bulk queries (@Modifying) modify the data directly in the database → so version is not updated.
To change the version at the same time as the Bulk Update, you need to use versioned in the JPQL query (Hibernate-specific).

We only want to display the names and salaries (name, salary) of the first three guides with salary > 2000; the user doesn't need to be able to change this data, and the order doesn't matter.
We can write a Query Method:
List<Guide> findFirst3BySalaryGreaterThan(Integer minSalary);
and call the service and print the result. Since we're only reading and not modifying, this should be done inside a read-only transaction:
@Transactional(readOnly = true)
public void displayNameAndSalaryOfFirst3Guides() {
var guides = guideRepository.findFirst3BySalaryGreaterThan(2000);
for (Guide g : guides) {
System.out.println(g.getName() + "\t" + g.getSalary());
}
}

}
Why readOnly?
If the transaction is read-write, JPA/Hibernate creates a snapshot for each entity to check if anything has changed in dirty checking; this consumes RAM/CPU. When we know that the data is read-only, @Transactional(readOnly = true) helps to avoid this overhead.
Drawbacks of this solution
With this approach, even though we only need name and salary, JPA loads the entire entity (all columns). This is I/O overhead and degrades performance; especially when the actual entities have a whole bunch of fields.
Optimal solution: Projections
Projection means fetching only a subset of the fields of an entity. In Spring Data JPA we have several models; this session focuses on Interface-based Projection.
1) Closed Projection
The interface only has getters for the fields in the entity:
public interface GuideNameSalary {
String getName();
Integer getSalary();
}
Now we change the return type of the repository method from List<Guide> to List<GuideNameSalary>:
List<GuideNameSalary> findFirst3BySalaryGreaterThan(Integer minSalary);
Result:
At runtime, Spring Data generates proxies of this interface that return only name and salary instead of the real entity, and SELECT only pulls these columns (instead of select g from Guide g ..., something like select g.name, g.salary from Guide g ...)
Important note: Since the entity is not loaded, entity callbacks like @PostLoad are not executed. Projections are also not managed, so you can rest assured about accidental changes.

2) Adding a calculated value to the Projection (Open Projection)
You want to return a new value that doesn't exist in the entity; for example, getInfo() that combines name and salary. This can be done with @Value and SpEL:
public interface GuideNameSalary {
String getName();
Integer getSalary();

@Value("#{target.name + '\t' + target.salary}")
String getInfo();
}
Now:
List<GuideNameSalary> findFirst3BySalaryGreaterThan(Integer minSalary);
...
for (GuideNameSalary g : list) {
System.out.println(g.getInfo());
}
Performance warning:
When using @Value/SpEL, Spring Data may need the entire entity to calculate the expression; so that SELECT optimization is no longer applied and all columns are loaded. This model is called Open Projection.
3) Overcoming Open Projection performance loss with @Query
To pull only the necessary columns again, we can write our own Query:
@Query(
value = "SELECT g.name AS name, g.salary AS salary " +
"FROM guide g WHERE g.salary > :minSalary " +
"ORDER BY g.id LIMIT 3",
nativeQuery = true
)
List<GuideNameSalary> findTop3NameSalary(@Param("minSalary") int minSalary);
Now even if we have a method like getInfo() in the projection, since we have explicitly selected the necessary columns, Query will pull only those and the entire entity will be prevented from loading.
Why nativeQuery = true? Because JPQL does not support LIMIT. Of course.

When you use @Value and target, target represents the entire underlying entity (here Guide).
SpEL can access any field of that entity, whether or not it is defined in the projection.
For example, if the entity Guide has these fields:
private String staffId;
private String name;
private Integer salary;
You can even write:
@Value("#{target.staffId + ' | ' + target.name + '\t' + target.salary}")
String getInfo();
Even if you don't define getStaffId() in your projection interface.
This feature prevents Spring Data from optimizing the SELECT query, because SpEL may access any field of the entity. So Hibernate loads all columns to be sure.
This means that when you use @Value/SpEL, even if you only write name and salary in the expression, extra columns will still be loaded and performance will decrease.


